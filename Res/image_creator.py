# Importing the libraries
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable

# Setting some hyperparameters
batchSize = 64 # Set the size of the batch.
imageSize = 64 # Set the size of the generated images (64x64).

# List of transformations (scaling, tensor conversion, normalization) to apply to the input images
transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])

# Loading the dataset
dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # Download the training set in the ./data folder and apply transformations on each image
dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # Get the images of the training set batch by batch by using dataLoader 

# Function takes as input a neural network m and will initialize all its weights
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

# Defining the generator

class G(nn.Module):

    def __init__(self):
        super(G, self).__init__() # Inherited from the nn.Module tools
        self.main = nn.Sequential( # Sequence of modules
            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # Inversed convolution
            nn.BatchNorm2d(512), # Normalize all the features along the dimension of the batch
            nn.ReLU(True), # Apply a ReLU rectification to break the linearity
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),
            nn.Tanh()
        )

    def forward(self, input):
        output = self.main(input) # Forward propagation the signal through NN of the generator defined by self.main
        return output # Returns the output of generated images

# Creating the generator
netG = G()
netG.apply(weights_init)

# Defining the discriminator

class D(nn.Module):

    def __init__(self):
        super(D, self).__init__() # Inherited from the nn.Module tools
        self.main = nn.Sequential( # Createinga meta module of NN; convolutions, full connections, etc
            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # Convolution
            nn.LeakyReLU(0.2, inplace = True), # Apply LeakyReLU
            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # Add another convolution
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(128, 256, 4, 2, 1, bias = False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(256, 512, 4, 2, 1, bias = False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(512, 1, 4, 1, 0, bias = False),
            nn.Sigmoid()
        )

    def forward(self, input):
        output = self.main(input) # Forward propagation the signal through NN of the **discriminator defined by self.main
        return output.view(-1) #Returns the output value 0-1

# Creating the discriminator
netD = D()
netD.apply(weights_init)

# Training the DCGANs

criterion = nn.BCELoss() # Measures the error between the prediction and the target
optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # Optimizer object of the discriminator
optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # Optimizer object of the generator

for epoch in range(25): # It will be 25 epochs

    for i, data in enumerate(dataloader, 0): # Iterate over the images of the dataset
        
        # Updating the weights of NN of the discriminator
        netD.zero_grad() # Initialize to 0
        
        # Training the discriminator with a real image of the dataset
        real, _ = data # Get real image of the dataset to use in training of discriminator
        input = Variable(real) # As a  variable
        target = Variable(torch.ones(input.size()[0])) # Get the target
        output = netD(input) # Forward propagate this real image into NN of the discriminator to get the prediction
        errD_real = criterion(output, target) # Compute the loss between predictions and the target
        
        # Training the discriminator with a fake image generated by the generator
        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) # Random input vector of the generator
        fake = netG(noise) # Forward propagate this random input vector into NN of the generator to get some fake generated images
        target = Variable(torch.zeros(input.size()[0])) # Get the target
        output = netD(fake.detach()) # Forward propagate the fake generated images into NN of the discriminator to get the prediction
        errD_fake = criterion(output, target) # Compute the error
        
        # Backpropagating the total error
        errD = errD_real + errD_fake # compute the total error of the discriminator
        errD.backward() # Backpropagate the loss error
        optimizerD.step() # Apply the optimizer to update the weights according

        # Updating the weights of the neural network of the generator

        netG.zero_grad() # Initialize to 0
        target = Variable(torch.ones(input.size()[0])) # Get the target.
        output = netD(fake) # Forward propagate the fake generated images into NN of the discriminator to get the prediction 
        errG = criterion(output, target) # Compute the loss between the prediction and the target 
        errG.backward() # Backpropagate the loss error
        optimizerG.step() # Apply the optimizer to update the weights
        
        
        # Printing the losses and saving the real images and the generated images of the minibatch every 100 steps

        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))
        if i % 100 == 0: # Every 100 steps:
            vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True)
            fake = netG(noise) # Get our fake generated images
            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True)
